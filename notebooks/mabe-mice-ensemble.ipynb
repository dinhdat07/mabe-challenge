{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb510ba",
   "metadata": {
    "papermill": {
     "duration": 0.006778,
     "end_time": "2025-12-10T10:37:25.283172",
     "exception": false,
     "start_time": "2025-12-10T10:37:25.276394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import and configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b900936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:25.295085Z",
     "iopub.status.busy": "2025-12-10T10:37:25.294791Z",
     "iopub.status.idle": "2025-12-10T10:37:27.916492Z",
     "shell.execute_reply": "2025-12-10T10:37:27.915626Z"
    },
    "papermill": {
     "duration": 2.62957,
     "end_time": "2025-12-10T10:37:27.918067",
     "exception": false,
     "start_time": "2025-12-10T10:37:25.288497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/dependencies/koolbox-0.1.3-py3-none-any.whl\r\n",
      "Installing collected packages: koolbox\r\n",
      "Successfully installed koolbox-0.1.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-deps /kaggle/input/dependencies/koolbox-0.1.3-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5b1ed7",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:27.930599Z",
     "iopub.status.busy": "2025-12-10T10:37:27.930357Z",
     "iopub.status.idle": "2025-12-10T10:37:29.871952Z",
     "shell.execute_reply": "2025-12-10T10:37:29.871326Z"
    },
    "papermill": {
     "duration": 1.949744,
     "end_time": "2025-12-10T10:37:29.873440",
     "exception": false,
     "start_time": "2025-12-10T10:37:27.923696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
    "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "    for row in lab_solution.to_dicts():\n",
    "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
    "\n",
    "    for video in lab_solution['video_id'].unique():\n",
    "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
    "        active_labels: set[str] = set(json.loads(active_labels))\n",
    "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
    "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
    "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
    "                continue\n",
    "\n",
    "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
    "            \n",
    "            # Ignore truly redundant predictions.\n",
    "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
    "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
    "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
    "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
    "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
    "            \n",
    "            prediction_frames[row['prediction_key']].update(new_frames)\n",
    "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
    "\n",
    "    tps = defaultdict(int)\n",
    "    fns = defaultdict(int)\n",
    "    fps = defaultdict(int)\n",
    "    for key, pred_frames in prediction_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        matched_label_frames = label_frames[key]\n",
    "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
    "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
    "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
    "\n",
    "    distinct_actions = set()\n",
    "    for key, frames in label_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        if key not in prediction_frames:\n",
    "            fns[action] += len(frames)\n",
    "\n",
    "    action_f1s = []\n",
    "    for action in distinct_actions:\n",
    "        if tps[action] + fns[action] + fps[action] == 0:\n",
    "            action_f1s.append(0)\n",
    "        else:\n",
    "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
    "    return sum(action_f1s) / len(action_f1s)\n",
    "\n",
    "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
    "\n",
    "    if len(solution) == 0 or len(submission) == 0:\n",
    "        raise ValueError('Missing solution or submission data')\n",
    "\n",
    "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "    for col in expected_cols:\n",
    "        if col not in solution.columns:\n",
    "            raise ValueError(f'Solution is missing column {col}')\n",
    "        if col not in submission.columns:\n",
    "            raise ValueError(f'Submission is missing column {col}')\n",
    "\n",
    "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
    "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
    "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
    "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
    "    solution_videos = set(solution['video_id'].unique())\n",
    "    \n",
    "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
    "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
    "\n",
    "    solution = solution.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key'),\n",
    "    )\n",
    "    submission = submission.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key'),\n",
    "    )\n",
    "\n",
    "    lab_scores = []\n",
    "    for lab in solution['lab_id'].unique():\n",
    "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
    "        lab_videos = set(lab_solution['video_id'].unique())\n",
    "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
    "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
    "\n",
    "    return sum(lab_scores) / len(lab_scores)\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
    "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    return mouse_fbeta(solution, submission, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ce20aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:29.886550Z",
     "iopub.status.busy": "2025-12-10T10:37:29.886244Z",
     "iopub.status.idle": "2025-12-10T10:37:35.489059Z",
     "shell.execute_reply": "2025-12-10T10:37:35.488492Z"
    },
    "papermill": {
     "duration": 5.611027,
     "end_time": "2025-12-10T10:37:35.490569",
     "exception": false,
     "start_time": "2025-12-10T10:37:29.879542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from koolbox import Trainer\n",
    "import numpy as np\n",
    "import itertools\n",
    "import optuna\n",
    "import warnings\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d21ca81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.503501Z",
     "iopub.status.busy": "2025-12-10T10:37:35.502805Z",
     "iopub.status.idle": "2025-12-10T10:37:35.507294Z",
     "shell.execute_reply": "2025-12-10T10:37:35.506641Z"
    },
    "papermill": {
     "duration": 0.011912,
     "end_time": "2025-12-10T10:37:35.508326",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.496414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n",
    "    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    model_path = \"/kaggle/input/my-model\"\n",
    "    \n",
    "    # mode = \"validate\"\n",
    "    mode = \"submit\"\n",
    "    \n",
    "    n_splits = 3\n",
    "    cv = StratifiedGroupKFold(n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e771d",
   "metadata": {
    "papermill": {
     "duration": 0.005522,
     "end_time": "2025-12-10T10:37:35.519338",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.513816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85533be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.531582Z",
     "iopub.status.busy": "2025-12-10T10:37:35.530946Z",
     "iopub.status.idle": "2025-12-10T10:37:35.675451Z",
     "shell.execute_reply": "2025-12-10T10:37:35.674829Z"
    },
    "papermill": {
     "duration": 0.152086,
     "end_time": "2025-12-10T10:37:35.676792",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.524706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "test = pd.read_csv(CFG.test_path)\n",
    "\n",
    "train[\"n_mice\"] = 4 - train[[\"mouse1_strain\", \"mouse2_strain\", \"mouse3_strain\", \"mouse4_strain\"]].isna().sum(axis=1)\n",
    "train_without_mbae = train.query(\"~lab_id.str.startswith('MABe22_')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6031eb9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.688977Z",
     "iopub.status.busy": "2025-12-10T10:37:35.688455Z",
     "iopub.status.idle": "2025-12-10T10:37:35.695173Z",
     "shell.execute_reply": "2025-12-10T10:37:35.694576Z"
    },
    "papermill": {
     "duration": 0.013906,
     "end_time": "2025-12-10T10:37:35.696250",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.682344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get unique raw entries\n",
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efd597",
   "metadata": {
    "papermill": {
     "duration": 0.005004,
     "end_time": "2025-12-10T10:37:35.706467",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.701463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating label dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9a1d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.718698Z",
     "iopub.status.busy": "2025-12-10T10:37:35.718273Z",
     "iopub.status.idle": "2025-12-10T10:37:35.724689Z",
     "shell.execute_reply": "2025-12-10T10:37:35.723933Z"
    },
    "papermill": {
     "duration": 0.013904,
     "end_time": "2025-12-10T10:37:35.725797",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.711893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_solution_df(dataset):\n",
    "    solution = []\n",
    "    missing_file = []\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        lab_id = row['lab_id']\n",
    "        \n",
    "        if lab_id.startswith('MABe22'): \n",
    "            continue\n",
    "            \n",
    "        video_id = row['video_id']\n",
    "        path = f\"{CFG.train_annotation_path}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            anno = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            missing_file.append(path)\n",
    "            continue\n",
    "\n",
    "        anno['lab_id'] = lab_id\n",
    "        anno['video_id'] = video_id\n",
    "        anno['behaviors_labeled'] = row['behaviors_labeled']\n",
    "        \n",
    "        anno['target_id'] = np.where(anno.target_id != anno.agent_id, anno['target_id'].apply(lambda s: f\"mouse{s}\"), 'self')\n",
    "        anno['agent_id'] = anno['agent_id'].apply(lambda s: f\"mouse{s}\")\n",
    "\n",
    "        solution.append(anno)\n",
    "    \n",
    "    solution = pd.concat(solution)\n",
    "    return solution, missing_file\n",
    "\n",
    "# use method above to create ground truth df\n",
    "if CFG.mode == 'validate':\n",
    "    solution, missing = create_solution_df(train_without_mbae)\n",
    "    logging.warning(\"Files not found:\")\n",
    "    print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d479e991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.737352Z",
     "iopub.status.busy": "2025-12-10T10:37:35.737125Z",
     "iopub.status.idle": "2025-12-10T10:37:35.752387Z",
     "shell.execute_reply": "2025-12-10T10:37:35.751634Z"
    },
    "papermill": {
     "duration": 0.022768,
     "end_time": "2025-12-10T10:37:35.753660",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.730892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DROP_BODY_PARTS = [\n",
    "    'headpiece_bottombackleft', 'headpiece_bottombackright',\n",
    "    'headpiece_bottomfrontleft', 'headpiece_bottomfrontright',\n",
    "    'headpiece_topbackleft', 'headpiece_topbackright',\n",
    "    'headpiece_topfrontleft', 'headpiece_topfrontright',\n",
    "    'spine_1', 'spine_2',\n",
    "    'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "]\n",
    "\n",
    "def generate_mouse_data(dataset, mode=None, is_train=True):\n",
    "    if is_train:\n",
    "        data_dir = CFG.train_tracking_path\n",
    "    else:\n",
    "        data_dir = CFG.test_tracking_path\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "\n",
    "        # skip if MABe lab or not string behaviors_labeled   \n",
    "        if lab_id.startswith(\"MABe22\") or not isinstance(row.behaviors_labeled, str):\n",
    "            continue\n",
    "\n",
    "        video_id = row.video_id\n",
    "        tracking_path =  f\"{data_dir}/{lab_id}/{video_id}.parquet\";\n",
    "\n",
    "        vid = pd.read_parquet(tracking_path)\n",
    "\n",
    "        # > 5 bodyparts -> drop\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~bodypart.isin(@DROP_BODY_PARTS)\")\n",
    "\n",
    "        # pivot\n",
    "        pvid = vid.pivot(\n",
    "            index=\"video_frame\",\n",
    "            columns=[\"mouse_id\", \"bodypart\"],\n",
    "            values=[\"x\", \"y\"],\n",
    "        )\n",
    "\n",
    "        # delete vid for memory\n",
    "        del vid\n",
    "        gc.collect()\n",
    "\n",
    "        # (coor, mouse, bodypart) -> (mouse, bodypart, coor) -> sorted columns \n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        \n",
    "        # pix to cm\n",
    "        pvid = pvid / row.pix_per_cm_approx\n",
    "        \n",
    "        # behaviors_labeled is JSON list\n",
    "        raw_behaviors = json.loads(row.behaviors_labeled)\n",
    "        \n",
    "        # remove ', duplicate by set then sort\n",
    "        cleaned = {b.replace(\"'\", \"\") for b in raw_behaviors} \n",
    "        cleaned = sorted(list(cleaned))\n",
    "\n",
    "        # split into 3 cols\n",
    "        behaviors_split = [b.split(\",\") for b in cleaned]\n",
    "        vid_beh = pd.DataFrame(behaviors_split, columns=[\"agent\", \"target\", \"action\"])\n",
    "\n",
    "        if is_train:\n",
    "            try: \n",
    "                anno_path = tracking_path.replace(\"train_tracking\", \"train_annotation\")\n",
    "                anno = pd.read_parquet(anno_path)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        \n",
    "        # ---- SINGLE MOUSE ----\n",
    "        if mode is None or mode == 'single' :\n",
    "            # only get target == self\n",
    "            vid_beh_single = vid_beh.query(\"target == 'self'\")\n",
    "\n",
    "            for agent_str in np.unique(vid_beh_single.agent):\n",
    "                try:\n",
    "                    # get the id (the last element of agent_str)\n",
    "                    mouse_id = int(agent_str[-1])\n",
    "                    \n",
    "                    # get all action of this agent \n",
    "                    agent_actions = np.unique(vid_beh_single.query(\"agent == @agent_str\").action)\n",
    "\n",
    "                    # get tracking of this agent\n",
    "                    single_mouse = pvid.loc[:, mouse_id] \n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                \n",
    "                    single_meta = pd.DataFrame({\n",
    "                        \"video_id\": video_id,\n",
    "                        \"agent_id\": agent_str,\n",
    "                        \"target_id\": \"self\",\n",
    "                        \"video_frame\": single_mouse.index, # index by frames\n",
    "                    })\n",
    "\n",
    "                    if is_train:\n",
    "                        single_label = pd.DataFrame(0.0, columns=agent_actions, index=single_mouse.index)\n",
    "                        anno_single = anno.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "\n",
    "                        for _, anno_row in anno_single.iterrows():\n",
    "                            start = anno_row['start_frame']\n",
    "                            end = anno_row['stop_frame']\n",
    "                            action = anno_row['action']\n",
    "                            single_label.loc[start:end, action] = 1.0\n",
    "\n",
    "                        yield \"single\", single_mouse, single_meta, single_label\n",
    "\n",
    "                    else:\n",
    "                        yield \"single\", single_mouse, single_meta, agent_actions\n",
    "                    \n",
    "                except KeyError:\n",
    "                    continue\n",
    "        \n",
    "        # ---- PAIR MOUSE ----\n",
    "        if mode is None or mode == 'pair':\n",
    "            # only get target != 'self'\n",
    "            vid_behaviors_pair = vid_beh.query(\"target != 'self'\")\n",
    "\n",
    "            if len(vid_behaviors_pair) == 0:\n",
    "                continue\n",
    "\n",
    "            # get list of mouse_ids\n",
    "            mouse_ids = np.unique(pvid.columns.get_level_values(\"mouse_id\"))\n",
    "\n",
    "            # permutation (agent, target) with agent != target\n",
    "            for agent_id, target_id in itertools.permutations(mouse_ids, 2):\n",
    "                agent_str = f\"mouse{agent_id}\"\n",
    "                target_str = f\"mouse{target_id}\"\n",
    "\n",
    "                # action of this (agent, target)\n",
    "                pair_actions = np.unique(\n",
    "                    vid_behaviors_pair.query(\"(agent == @agent_str) & (target == @target_str)\").action\n",
    "                )\n",
    "\n",
    "                # tracking of these 2 mice\n",
    "                mouse_pair = pd.concat(\n",
    "                    [pvid[agent_id], pvid[target_id]],\n",
    "                    axis=1,\n",
    "                    keys=[\"A\", \"B\"],  # A = agent, B = target\n",
    "                )\n",
    "                assert len(mouse_pair) == len(pvid)\n",
    "\n",
    "                # metadata \n",
    "                pair_meta = pd.DataFrame({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_str,\n",
    "                    \"target_id\": target_str,\n",
    "                    \"video_frame\": mouse_pair.index,\n",
    "                })\n",
    "\n",
    "                if is_train:\n",
    "                    # label: frame × action\n",
    "                    pair_label = pd.DataFrame(0.0, columns=pair_actions, index=mouse_pair.index)\n",
    "                    anno_pair = anno.query(\n",
    "                        \"(agent_id == @agent_id) & (target_id == @target_id)\"\n",
    "                    )\n",
    "\n",
    "                    for _, anno_row in anno_pair.iterrows():\n",
    "                        start = anno_row[\"start_frame\"]\n",
    "                        end = anno_row[\"stop_frame\"]\n",
    "                        action = anno_row[\"action\"]\n",
    "                        pair_label.loc[start:end, action] = 1.0\n",
    "\n",
    "                    yield \"pair\", mouse_pair, pair_meta, pair_label\n",
    "\n",
    "                else:\n",
    "                    # test/val: list action\n",
    "                    yield \"pair\", mouse_pair, pair_meta, pair_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8965a0",
   "metadata": {
    "papermill": {
     "duration": 0.005381,
     "end_time": "2025-12-10T10:37:35.764256",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.758875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c4116d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.775501Z",
     "iopub.status.busy": "2025-12-10T10:37:35.775117Z",
     "iopub.status.idle": "2025-12-10T10:37:35.781332Z",
     "shell.execute_reply": "2025-12-10T10:37:35.780715Z"
    },
    "papermill": {
     "duration": 0.013063,
     "end_time": "2025-12-10T10:37:35.782333",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.769270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper\n",
    "\n",
    "def scaled_window(n_frames_30fps , fps , min_frac=0.2, min_abs=1):\n",
    "    ws = max(1, int(round(n_frames_30fps * float(fps) / 30.0)))\n",
    "    min_periods = max(min_abs, int(round(ws * min_frac)))\n",
    "    return ws, min_periods\n",
    "\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup: dict, default_fps: float = 30.0):\n",
    "    if \"frames_per_second\" in meta_df.columns and pd.notnull(meta_df[\"frames_per_second\"]).any():\n",
    "        return float(meta_df[\"frames_per_second\"].iloc[0])\n",
    "    vid = meta_df[\"video_id\"].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453fd6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.794107Z",
     "iopub.status.busy": "2025-12-10T10:37:35.793520Z",
     "iopub.status.idle": "2025-12-10T10:37:35.807012Z",
     "shell.execute_reply": "2025-12-10T10:37:35.806180Z"
    },
    "papermill": {
     "duration": 0.02092,
     "end_time": "2025-12-10T10:37:35.808296",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.787376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature for each mouse\n",
    "\n",
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    # velocity & acceleration\n",
    "    vx = center_x.diff()\n",
    "    vy = center_y.diff()\n",
    "    ax = vx.diff()\n",
    "    ay = vy.diff()\n",
    "\n",
    "    # curve ~ |v × a| / |v|^3\n",
    "    cross = vx * ay - vy * ax\n",
    "    speed = np.sqrt(vx**2 + vy**2)\n",
    "    curvature = np.abs(cross) / (speed**3 + 1e-6)\n",
    "\n",
    "    # avg curve in scales\n",
    "    for base_w in [25, 50, 75]:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=0.2)\n",
    "        X[f\"curv_mean_{base_w}\"] = curvature.rolling(ws, min_periods=mp).mean()\n",
    "    \n",
    "    angle = np.arctan2(vy, vx)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    ws, mp = scaled_window(30, fps, min_frac=0.2)\n",
    "    X[\"turn_rate_30\"] = angle_change.rolling(ws, min_periods=mp).sum()\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for base_w in scales:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=0.25)\n",
    "        if len(speed) >= ws:\n",
    "            X[f\"sp_m{base_w}\"] = speed.rolling(ws, min_periods=mp).mean()\n",
    "            X[f\"sp_s{base_w}\"] = speed.rolling(ws, min_periods=mp).std()\n",
    "\n",
    "    if all(f\"sp_m{s}\" in X.columns for s in (scales[0], scales[-1])):\n",
    "        X[\"sp_ratio\"] = X[f\"sp_m{scales[0]}\"] / (X[f\"sp_m{scales[-1]}\"] + 1e-6)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    ws_ma, mp_ma = scaled_window(15, fps, min_frac=1/3)\n",
    "    speed_ma = speed.rolling(ws_ma, min_periods=mp_ma).mean()\n",
    "\n",
    "    try:\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        for base_w in [20, 40, 60, 80]:\n",
    "            ws, mp = scaled_window(base_w, fps, min_frac=0.2)\n",
    "            if len(speed_states) < ws:\n",
    "                continue\n",
    "\n",
    "            for state in [0, 1, 2, 3]:\n",
    "                X[f\"s{state}_{base_w}\"] = (\n",
    "                    (speed_states == state)\n",
    "                    .astype(float)\n",
    "                    .rolling(ws, min_periods=mp)\n",
    "                    .mean()\n",
    "                )\n",
    "\n",
    "            state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "            X[f\"trans_{base_w}\"] = state_changes.rolling(ws, min_periods=mp).sum()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    # longrange moving average\n",
    "    for base_w in [30, 60, 120]:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=1/6, min_abs=5)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f\"x_ml{base_w}\"] = center_x.rolling(ws, min_periods=mp).mean()\n",
    "            X[f\"y_ml{base_w}\"] = center_y.rolling(ws, min_periods=mp).mean()\n",
    "        \n",
    "    # EWMA \n",
    "    for span in [30, 60, 120]:\n",
    "        s, _ = scaled_window(span, fps, min_frac=0.0)  # min_periods sẽ set riêng\n",
    "        X[f\"x_e{span}\"] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f\"y_e{span}\"] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    # percentile rank of speed\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    for base_w in [30, 60, 120]:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=1 / 6, min_abs=5)\n",
    "        if len(speed) >= ws:\n",
    "            X[f\"sp_pct{base_w}\"] = speed.rolling(ws, min_periods=mp).rank(pct=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c14a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.819458Z",
     "iopub.status.busy": "2025-12-10T10:37:35.819249Z",
     "iopub.status.idle": "2025-12-10T10:37:35.826910Z",
     "shell.execute_reply": "2025-12-10T10:37:35.826101Z"
    },
    "papermill": {
     "duration": 0.014602,
     "end_time": "2025-12-10T10:37:35.828089",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.813487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_single_extra_features(X, single_mouse, available_parts, fps):\n",
    "    # posture curvature\n",
    "    if all(p in available_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        # body_center - tail_base vs nose - body_center\n",
    "        v1 = single_mouse['body_center'] - single_mouse['tail_base']\n",
    "        v2 = single_mouse['nose'] - single_mouse['body_center']\n",
    "\n",
    "        dot = v1['x'] * v2['x'] + v1['y'] * v2['y']\n",
    "        n1 = np.sqrt(v1['x']**2 + v1['y']**2)\n",
    "        n2 = np.sqrt(v2['x']**2 + v2['y']**2)\n",
    "\n",
    "        X['pose_curv'] = (dot / (n1 * n2 + 1e-6)).astype(np.float32)\n",
    "\n",
    "    # verticality/ rearing proxy\n",
    "    if all(p in available_parts for p in ['nose', 'lateral_left', 'lateral_right']):\n",
    "        nose_x = single_mouse['nose']['x']\n",
    "        nose_y = single_mouse['nose']['y']\n",
    "        lat_x = (single_mouse['lateral_left']['x'] + single_mouse['lateral_right']['x']) / 2.0\n",
    "        lat_y = (single_mouse['lateral_left']['y'] + single_mouse['lateral_right']['y']) / 2.0\n",
    "\n",
    "        nose_lat_dist = np.sqrt((nose_x - lat_x)**2 + (nose_y - lat_y)**2)\n",
    "        X[\"nose_lateral_dist\"] = nose_lat_dist.astype(np.float32)\n",
    "        X[\"nose_lateral_vel\"] = nose_lat_dist.diff().astype(np.float32)\n",
    "\n",
    "    # ear dynamics\n",
    "    if all(p in available_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt(\n",
    "            (single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 +\n",
    "            (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2\n",
    "        )\n",
    "        X[\"ear_vel\"] = ear_d.diff().astype(np.float32)\n",
    "        X[\"ear_acc\"] = ear_d.diff().diff().astype(np.float32)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9233c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.840056Z",
     "iopub.status.busy": "2025-12-10T10:37:35.839806Z",
     "iopub.status.idle": "2025-12-10T10:37:35.856239Z",
     "shell.execute_reply": "2025-12-10T10:37:35.855635Z"
    },
    "papermill": {
     "duration": 0.024002,
     "end_time": "2025-12-10T10:37:35.857349",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.833347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    available_parts = single_mouse.columns.get_level_values(0)\n",
    "\n",
    "    # pairwise distance between body part (p1, p2) ---\n",
    "    features = {}\n",
    "\n",
    "    for p1, p2 in itertools.combinations(body_parts_tracked, 2):\n",
    "        if p1 in available_parts and p2 in available_parts:\n",
    "            diff = single_mouse[p1] - single_mouse[p2]      # (x,y) or (x,y,...) by frame\n",
    "            dist2 = np.square(diff).sum(axis=1, skipna=False)\n",
    "            features[f\"{p1}+{p2}\"] = dist2\n",
    "\n",
    "    X = pd.DataFrame(features)\n",
    "\n",
    "    # ensure order\n",
    "    full_cols = [f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)]\n",
    "    X = X.reindex(columns=full_cols, copy=False)\n",
    "\n",
    "    # raw speed by ear and tail (lag ~10 frame) ---\n",
    "    if all(p in available_parts for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        lag = _scale(10, fps)\n",
    "        past = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n",
    "\n",
    "        sp_lf  = np.square(single_mouse['ear_left']  - past['ear_left']).sum(axis=1, skipna=False)\n",
    "        sp_rt  = np.square(single_mouse['ear_right'] - past['ear_right']).sum(axis=1, skipna=False)\n",
    "        sp_lf2 = np.square(single_mouse['ear_left']  - past['tail_base']).sum(axis=1, skipna=False)\n",
    "        sp_rt2 = np.square(single_mouse['ear_right'] - past['tail_base']).sum(axis=1, skipna=False)\n",
    "\n",
    "        X[['sp_lf', 'sp_rt', 'sp_lf2', 'sp_rt2']] = np.column_stack([sp_lf, sp_rt, sp_lf2, sp_rt2])\n",
    "\n",
    "    # elongation\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    # body angle\n",
    "    if all(p in available_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose']      - single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "\n",
    "        dot = v1['x'] * v2['x'] + v1['y'] * v2['y']\n",
    "        norm1 = np.sqrt(v1['x']**2 + v1['y']**2)\n",
    "        norm2 = np.sqrt(v2['x']**2 + v2['y']**2)\n",
    "        body_ang = dot / (norm1 * norm2 + 1e-6)\n",
    "\n",
    "        X['body_ang'] = body_ang.astype(np.float32)\n",
    "        X['body_ang_vel'] = body_ang.diff().astype(np.float32)\n",
    "        X['body_ang_acc'] = body_ang.diff().diff().astype(np.float32)\n",
    "\n",
    "    # features by body_center\n",
    "    if 'body_center' in available_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "\n",
    "        for base_w in [5, 15, 30, 60]:\n",
    "            ws = _scale(base_w, fps)\n",
    "            roll_kwargs = dict(window=ws, min_periods=1, center=True)\n",
    "\n",
    "            X[f'cx_m{base_w}'] = cx.rolling(**roll_kwargs).mean()\n",
    "            X[f'cy_m{base_w}'] = cy.rolling(**roll_kwargs).mean()\n",
    "            X[f'cx_s{base_w}'] = cx.rolling(**roll_kwargs).std()\n",
    "            X[f'cy_s{base_w}'] = cy.rolling(**roll_kwargs).std()\n",
    "\n",
    "            X[f'x_rng{base_w}'] = cx.rolling(**roll_kwargs).max() - cx.rolling(**roll_kwargs).min()\n",
    "            X[f'y_rng{base_w}'] = cy.rolling(**roll_kwargs).max() - cy.rolling(**roll_kwargs).min()\n",
    "\n",
    "            # displacement & activity (from diff)\n",
    "            dx = cx.diff()\n",
    "            dy = cy.diff()\n",
    "            disp = np.sqrt(dx.rolling(ws, min_periods=1).sum()**2 + dy.rolling(ws, min_periods=1).sum()**2)\n",
    "            act = np.sqrt(dx.rolling(ws, min_periods=1).var() + dy.rolling(ws, min_periods=1).var())\n",
    "            X[f'disp{base_w}'] = disp\n",
    "            X[f'act{base_w}']  = act\n",
    "\n",
    "        # advanced feature\n",
    "        X = add_curvature_features(X, cx, cy, fps)\n",
    "        X = add_multiscale_features(X, cx, cy, fps)\n",
    "        X = add_state_features(X, cx, cy, fps)\n",
    "        X = add_longrange_features(X, cx, cy, fps)\n",
    "\n",
    "    # nose–tail_based distance by time \n",
    "    if all(p in available_parts for p in ['nose', 'tail_base']):\n",
    "        nt = np.sqrt(\n",
    "            (single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n",
    "            (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2\n",
    "        )\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nt_lg{lag}'] = nt.shift(l)\n",
    "            X[f'nt_df{lag}'] = nt - nt.shift(l)\n",
    "\n",
    "    # ear distance & consistency\n",
    "    if all(p in available_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt(\n",
    "            (single_mouse['ear_left']['x']   - single_mouse['ear_right']['x'])**2 +\n",
    "            (single_mouse['ear_left']['y']   - single_mouse['ear_right']['y'])**2\n",
    "        )\n",
    "        # offset\n",
    "        for off in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "\n",
    "        w = _scale(30, fps)\n",
    "        roll_c = dict(window=w, min_periods=1, center=True)\n",
    "        ear_mean = ear_d.rolling(**roll_c).mean()\n",
    "        ear_std  = ear_d.rolling(**roll_c).std()\n",
    "        X['ear_con'] = ear_std / (ear_mean + 1e-6)\n",
    "\n",
    "    X = add_single_extra_features(X, single_mouse, available_parts, fps)\n",
    "\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2f366b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.869551Z",
     "iopub.status.busy": "2025-12-10T10:37:35.869124Z",
     "iopub.status.idle": "2025-12-10T10:37:35.876799Z",
     "shell.execute_reply": "2025-12-10T10:37:35.876150Z"
    },
    "papermill": {
     "duration": 0.015091,
     "end_time": "2025-12-10T10:37:35.877963",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.862872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature for mice interaction\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    if \"body_center\" not in avail_A or \"body_center\" not in avail_B:\n",
    "        return X\n",
    "    \n",
    "    # coor\n",
    "    Ax = mouse_pair[\"A\"][\"body_center\"][\"x\"]\n",
    "    Ay = mouse_pair[\"A\"][\"body_center\"][\"y\"]\n",
    "    Bx = mouse_pair[\"B\"][\"body_center\"][\"x\"]\n",
    "    By = mouse_pair[\"B\"][\"body_center\"][\"y\"]\n",
    "\n",
    "    # relative \n",
    "    rel_x = Ax - Bx\n",
    "    rel_y = Ay - By\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    # velocity\n",
    "    A_vx = Ax.diff()\n",
    "    A_vy = Ay.diff()\n",
    "    B_vx = Bx.diff()\n",
    "    B_vy = By.diff()\n",
    "\n",
    "    # cosine angle between vector (A, B) and velocity vector\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    # avg of A_lead, B_lead on windows\n",
    "    for base_w in [30, 60]:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=1 / 6)\n",
    "        X[f\"A_ld{base_w}\"] = A_lead.rolling(ws, min_periods=mp).mean()\n",
    "        X[f\"B_ld{base_w}\"] = B_lead.rolling(ws, min_periods=mp).mean()\n",
    "    \n",
    "    # approach\n",
    "    approach = -rel_dist.diff()\n",
    "    chase = approach * B_lead\n",
    "    ws, mp = scaled_window(30, fps, min_frac=1/6)\n",
    "    X[\"chase_30\"] = chase.rolling(ws, min_periods=mp).mean()\n",
    "\n",
    "    # correlation of 2 mice speed in long windows\n",
    "    A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "    B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "    for base_w in [60, 120]:\n",
    "        ws, mp = scaled_window(base_w, fps, min_frac=1 / 6)\n",
    "        X[f\"sp_cor{base_w}\"] = A_sp.rolling(ws, min_periods=mp).corr(B_sp)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece16d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.890116Z",
     "iopub.status.busy": "2025-12-10T10:37:35.889877Z",
     "iopub.status.idle": "2025-12-10T10:37:35.899230Z",
     "shell.execute_reply": "2025-12-10T10:37:35.898653Z"
    },
    "papermill": {
     "duration": 0.017025,
     "end_time": "2025-12-10T10:37:35.900358",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.883333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_egocentric_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    # cjeck condition\n",
    "    ok_A = all(p in avail_A for p in ['nose', 'tail_base', 'body_center'])\n",
    "    ok_B = all(p in avail_B for p in ['nose', 'tail_base', 'body_center'])\n",
    "    if not (ok_A and ok_B):\n",
    "        return X\n",
    "\n",
    "    # position\n",
    "    Ax = mouse_pair['A']['body_center']['x']\n",
    "    Ay = mouse_pair['A']['body_center']['y']\n",
    "    Bx = mouse_pair['B']['body_center']['x']\n",
    "    By = mouse_pair['B']['body_center']['y']\n",
    "\n",
    "    # head direction of A/B\n",
    "    headA_x = mouse_pair['A']['nose']['x'] - mouse_pair['A']['tail_base']['x']\n",
    "    headA_y = mouse_pair['A']['nose']['y'] - mouse_pair['A']['tail_base']['y']\n",
    "    headB_x = mouse_pair['B']['nose']['x'] - mouse_pair['B']['tail_base']['x']\n",
    "    headB_y = mouse_pair['B']['nose']['y'] - mouse_pair['B']['tail_base']['y']\n",
    "\n",
    "    # vector A → B\n",
    "    relAB_x = Bx - Ax\n",
    "    relAB_y = By - Ay\n",
    "\n",
    "    # cos / sin angle between A and vector A→B\n",
    "    dotA = headA_x * relAB_x + headA_y * relAB_y\n",
    "    norm_headA = np.sqrt(headA_x**2 + headA_y**2) + 1e-6\n",
    "    norm_relAB = np.sqrt(relAB_x**2 + relAB_y**2) + 1e-6\n",
    "\n",
    "    cos_bearing_A = dotA / (norm_headA * norm_relAB)\n",
    "    # sign of cross product → trái/phải\n",
    "    crossA = headA_x * relAB_y - headA_y * relAB_x\n",
    "    sin_bearing_A = crossA / (norm_headA * norm_relAB)\n",
    "\n",
    "    X['A_bearing_cos'] = cos_bearing_A.astype(np.float32)\n",
    "    X['A_bearing_sin'] = sin_bearing_A.astype(np.float32)\n",
    "\n",
    "    # for B\n",
    "    relBA_x = Ax - Bx\n",
    "    relBA_y = Ay - By\n",
    "    dotB = headB_x * relBA_x + headB_y * relBA_y\n",
    "    norm_headB = np.sqrt(headB_x**2 + headB_y**2) + 1e-6\n",
    "    norm_relBA = np.sqrt(relBA_x**2 + relBA_y**2) + 1e-6\n",
    "\n",
    "    cos_bearing_B = dotB / (norm_headB * norm_relBA)\n",
    "    crossB = headB_x * relBA_y - headB_y * relBA_x\n",
    "    sin_bearing_B = crossB / (norm_headB * norm_relBA)\n",
    "\n",
    "    X['B_bearing_cos'] = cos_bearing_B.astype(np.float32)\n",
    "    X['B_bearing_sin'] = sin_bearing_B.astype(np.float32)\n",
    "\n",
    "    # rolling stats \n",
    "    for base_w in [15, 30]:\n",
    "        ws = _scale(base_w, fps)\n",
    "        roll = dict(window=ws, min_periods=1, center=True)\n",
    "        X[f'A_bearing_cos_m{base_w}'] = X['A_bearing_cos'].rolling(**roll).mean()\n",
    "        X[f'A_bearing_sin_m{base_w}'] = X['A_bearing_sin'].rolling(**roll).mean()\n",
    "        X[f'B_bearing_cos_m{base_w}'] = X['B_bearing_cos'].rolling(**roll).mean()\n",
    "        X[f'B_bearing_sin_m{base_w}'] = X['B_bearing_sin'].rolling(**roll).mean()\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72381ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.912330Z",
     "iopub.status.busy": "2025-12-10T10:37:35.912115Z",
     "iopub.status.idle": "2025-12-10T10:37:35.918589Z",
     "shell.execute_reply": "2025-12-10T10:37:35.917846Z"
    },
    "papermill": {
     "duration": 0.014036,
     "end_time": "2025-12-10T10:37:35.919766",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.905730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_asymmetry_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    # need body_center to define speeds\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    Ax = mouse_pair['A']['body_center']['x']\n",
    "    Ay = mouse_pair['A']['body_center']['y']\n",
    "    Bx = mouse_pair['B']['body_center']['x']\n",
    "    By = mouse_pair['B']['body_center']['y']\n",
    "\n",
    "    # velocities (frame-wise differences)\n",
    "    A_vx = Ax.diff()\n",
    "    A_vy = Ay.diff()\n",
    "    B_vx = Bx.diff()\n",
    "    B_vy = By.diff()\n",
    "\n",
    "    # instantaneous speeds\n",
    "    A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "    B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "\n",
    "    # asymmetry: difference and ratio\n",
    "    sp_diff = A_sp - B_sp\n",
    "    sp_ratio = A_sp / (B_sp + 1e-6)\n",
    "\n",
    "    X['sp_diff_inst'] = sp_diff.astype(np.float32)\n",
    "    X['sp_ratio_inst'] = sp_ratio.astype(np.float32)\n",
    "\n",
    "    # rolling stats over short/medium windows\n",
    "    for base_w in [30, 60]:\n",
    "        ws = _scale(base_w, fps)\n",
    "        roll = dict(window=ws, min_periods=1, center=True)\n",
    "\n",
    "        X[f'sp_diff_m{base_w}'] = (\n",
    "            sp_diff.rolling(**roll).mean().astype(np.float32)\n",
    "        )\n",
    "        X[f'sp_ratio_m{base_w}'] = (\n",
    "            sp_ratio.rolling(**roll).mean().astype(np.float32)\n",
    "        )\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8fabbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.931728Z",
     "iopub.status.busy": "2025-12-10T10:37:35.931487Z",
     "iopub.status.idle": "2025-12-10T10:37:35.953827Z",
     "shell.execute_reply": "2025-12-10T10:37:35.953029Z"
    },
    "papermill": {
     "duration": 0.030017,
     "end_time": "2025-12-10T10:37:35.955089",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.925072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    avail_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    avail_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "\n",
    "    # pairwise distance between A[p1] and B[p2]\n",
    "    features = {}\n",
    "    for p1, p2 in itertools.product(body_parts_tracked, repeat=2):\n",
    "        if p1 in avail_A and p2 in avail_B:\n",
    "            diff = mouse_pair['A'][p1] - mouse_pair['B'][p2]\n",
    "            dist2 = np.square(diff).sum(axis=1, skipna=False)\n",
    "            features[f\"12+{p1}+{p2}\"] = dist2\n",
    "\n",
    "    X = pd.DataFrame(features)\n",
    "    full_cols = [f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)]\n",
    "    X = X.reindex(columns=full_cols, copy=False)\n",
    "\n",
    "    # ear-left speed A/B (lag ~10 frame)\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shA = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shB = mouse_pair['B']['ear_left'].shift(lag)\n",
    "\n",
    "        sp_A  = np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False)\n",
    "        sp_AB = np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False)\n",
    "        sp_B  = np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False)\n",
    "\n",
    "        X[['sp_A', 'sp_AB', 'sp_B']] = np.column_stack([sp_A, sp_AB, sp_B])\n",
    "\n",
    "    # elong = dist(nose, tail_base) / dist(ear_left, ear_right)\n",
    "    have_A_elong = all(p in avail_A for p in ['nose', 'tail_base', 'ear_left', 'ear_right'])\n",
    "    have_B_elong = all(p in avail_B for p in ['nose', 'tail_base', 'ear_left', 'ear_right'])\n",
    "\n",
    "    if have_A_elong:\n",
    "        nose_A = mouse_pair['A']['nose']\n",
    "        tail_A = mouse_pair['A']['tail_base']\n",
    "        el_A_l = mouse_pair['A']['ear_left']\n",
    "        el_A_r = mouse_pair['A']['ear_right']\n",
    "\n",
    "        nose_tail_A = np.square(nose_A - tail_A).sum(axis=1, skipna=False)\n",
    "        ear_dist_A  = np.square(el_A_l - el_A_r).sum(axis=1, skipna=False)\n",
    "        X['elong_A'] = nose_tail_A / (ear_dist_A + 1e-6)\n",
    "\n",
    "    if have_B_elong:\n",
    "        nose_B = mouse_pair['B']['nose']\n",
    "        tail_B = mouse_pair['B']['tail_base']\n",
    "        el_B_l = mouse_pair['B']['ear_left']\n",
    "        el_B_r = mouse_pair['B']['ear_right']\n",
    "\n",
    "        nose_tail_B = np.square(nose_B - tail_B).sum(axis=1, skipna=False)\n",
    "        ear_dist_B  = np.square(el_B_l - el_B_r).sum(axis=1, skipna=False)\n",
    "        X['elong_B'] = nose_tail_B / (ear_dist_B + 1e-6)\n",
    "\n",
    "    # diff and ratio\n",
    "    if 'elong_A' in X.columns and 'elong_B' in X.columns:\n",
    "        X['elong_diff']  = X['elong_A'] - X['elong_B']\n",
    "        X['elong_ratio'] = X['elong_A'] / (X['elong_B'] + 1e-6)\n",
    "\n",
    "    # relative body angle between A and B\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "\n",
    "        dot = dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']\n",
    "        nA = np.sqrt(dir_A['x']**2 + dir_A['y']**2)\n",
    "        nB = np.sqrt(dir_B['x']**2 + dir_B['y']**2)\n",
    "        X['rel_ori'] = dot / (nA * nB + 1e-6)\n",
    "\n",
    "    # nose-nose approach\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn_cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n",
    "        lag = _scale(10, fps)\n",
    "        shA_n = mouse_pair['A']['nose'].shift(lag)\n",
    "        shB_n = mouse_pair['B']['nose'].shift(lag)\n",
    "        nn_past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n",
    "        X['appr'] = nn_cur - nn_past\n",
    "\n",
    "    # distance categories by body_center\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        Ax = mouse_pair['A']['body_center']['x']\n",
    "        Ay = mouse_pair['A']['body_center']['y']\n",
    "        Bx = mouse_pair['B']['body_center']['x']\n",
    "        By = mouse_pair['B']['body_center']['y']\n",
    "\n",
    "        cd = np.sqrt((Ax - Bx)**2 + (Ay - By)**2)\n",
    "\n",
    "        X['v_cls'] = (cd < 5.0).astype(float)\n",
    "        X['cls']   = ((cd >= 5.0)  & (cd < 15.0)).astype(float)\n",
    "        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "        X['far']   = (cd >= 30.0).astype(float)\n",
    "\n",
    "        # stats on squared distance\n",
    "        cd2 = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "        \n",
    "        for base_w in [5, 15, 30, 60]:\n",
    "            ws = _scale(base_w, fps)\n",
    "            roll_c = dict(window=ws, min_periods=1, center=True)\n",
    "    \n",
    "            X[f'd_m{base_w}']  = cd2.rolling(**roll_c).mean()\n",
    "            X[f'd_s{base_w}']  = cd2.rolling(**roll_c).std()\n",
    "            X[f'd_mn{base_w}'] = cd2.rolling(**roll_c).min()\n",
    "            X[f'd_mx{base_w}'] = cd2.rolling(**roll_c).max()\n",
    "    \n",
    "            d_var = cd2.rolling(**roll_c).var()\n",
    "            X[f'int{base_w}'] = 1.0 / (1.0 + d_var)\n",
    "    \n",
    "            # dot product vận tốc body_center\n",
    "            Axd = Ax.diff()\n",
    "            Ayd = Ay.diff()\n",
    "            Bxd = Bx.diff()\n",
    "            Byd = By.diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{base_w}'] = coord.rolling(**roll_c).mean()\n",
    "            X[f'co_s{base_w}'] = coord.rolling(**roll_c).std()\n",
    "\n",
    "            # cosine similarity A,B speed (offset) ---\n",
    "            Avx = Ax.diff()\n",
    "            Avy = Ay.diff()\n",
    "            Bvx = Bx.diff()\n",
    "            Bvy = By.diff()\n",
    "            vel_cos = (Avx * Bvx + Avy * Bvy) / (\n",
    "                np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6\n",
    "            )\n",
    "        \n",
    "            for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "                o = _scale_signed(off, fps)\n",
    "                X[f'va_{off}'] = vel_cos.shift(-o)\n",
    "        \n",
    "            w = _scale(30, fps)\n",
    "            roll_c30 = dict(window=w, min_periods=1, center=True)\n",
    "            cd2_mean = cd2.rolling(**roll_c30).mean()\n",
    "            cd2_std  = cd2.rolling(**roll_c30).std()\n",
    "            X['int_con'] = cd2_std / (cd2_mean + 1e-6)\n",
    "\n",
    "            # advanced features\n",
    "            X = add_asymmetry_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "            X = add_egocentric_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "            X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "\n",
    "    # nose-nose distance + close percentage\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn = np.sqrt(\n",
    "            (mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n",
    "            (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2\n",
    "        )\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}'] = nn.shift(l)\n",
    "            X[f'nn_ch{lag}'] = nn - nn.shift(l)\n",
    "            is_close = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}'] = is_close.rolling(l, min_periods=1).mean()\n",
    "    \n",
    "\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42877da3",
   "metadata": {
    "papermill": {
     "duration": 0.005478,
     "end_time": "2025-12-10T10:37:35.966074",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.960596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e62b703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:35.978044Z",
     "iopub.status.busy": "2025-12-10T10:37:35.977825Z",
     "iopub.status.idle": "2025-12-10T10:37:35.988801Z",
     "shell.execute_reply": "2025-12-10T10:37:35.987973Z"
    },
    "papermill": {
     "duration": 0.018473,
     "end_time": "2025-12-10T10:37:35.990040",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.971567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_and_fill_submission(submission, meta_df, is_train=True):\n",
    "    if is_train:\n",
    "        tracking_dir = CFG.train_tracking_path\n",
    "    else: \n",
    "        tracking_dir = CFG.test_tracking_path\n",
    "    \n",
    "    # remove where start >= stop\n",
    "    prev_len = len(submission)\n",
    "    submission = submission[submission['start_frame'] < submission['stop_frame']].copy()\n",
    "    if len(submission) != prev_len:\n",
    "        print(\"Dropped rows with start_frame > stop_frame\")\n",
    "    \n",
    "    # remove overlap\n",
    "    prev_len = len(submission)\n",
    "    cleaned_groups = []\n",
    "\n",
    "    for (_, grp) in submission.groupby(['video_id', 'agent_id', 'target_id']):\n",
    "        grp = grp.sort_values('start_frame')\n",
    "        keep_mask = np.ones(len(grp), dtype=bool)\n",
    "\n",
    "        last_stop = -1\n",
    "        for i, (_, row) in enumerate(grp.iterrows()):\n",
    "            if row['start_frame'] < last_stop:\n",
    "                keep_mask[i] = False\n",
    "            else:\n",
    "                last_stop = row['stop_frame']\n",
    "        \n",
    "        cleaned_groups.append(grp[keep_mask])\n",
    "\n",
    "    submission = pd.concat(cleaned_groups, ignore_index=True)\n",
    "    if len(submission) != prev_len:\n",
    "        print(\"Dropped rows with overlapped intervals\")   \n",
    "\n",
    "    # dummy prediction for video have no prediction\n",
    "    dummy_rows = []\n",
    "\n",
    "    for _, row in meta_df.iterrows():\n",
    "        lab_id = row[\"lab_id\"]\n",
    "\n",
    "        # remove MABe22 vids\n",
    "        if isinstance(lab_id, str) and lab_id.startswith(\"MABe22\"):\n",
    "            continue\n",
    "        \n",
    "        # remove behaviors_labeled if not string\n",
    "        if not isinstance(row.get(\"behaviors_labeled\", None), str):\n",
    "            continue\n",
    "\n",
    "        video_id = row[\"video_id\"]\n",
    "\n",
    "        # if have prediction -> skip\n",
    "        if (submission[\"video_id\"] == video_id).any():\n",
    "            continue\n",
    "\n",
    "        print(f\"Video {video_id} has no predictions. Filling dummy segments...\")\n",
    "\n",
    "        # read tracking\n",
    "        path = f\"{tracking_dir}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "\n",
    "        # get list (agent, target, action) from meta\n",
    "        raw_behaviors = json.loads(row[\"behaviors_labeled\"])\n",
    "        cleaned = {b.replace(\"'\", \"\") for b in raw_behaviors}\n",
    "        triplets = [b.split(\",\") for b in sorted(cleaned)]\n",
    "        beh_df = pd.DataFrame(triplets, columns=[\"agent\", \"target\", \"action\"])\n",
    "\n",
    "        # get total frames of this video\n",
    "        start_frame = vid[\"video_frame\"].min()\n",
    "        stop_frame = vid[\"video_frame\"].max() + 1\n",
    "        total_frames = stop_frame - start_frame\n",
    "\n",
    "        # divide uniformly \n",
    "        for (agent, target), actions in beh_df.groupby([\"agent\", \"target\"]):\n",
    "            n_actions = len(actions)\n",
    "            if n_actions == 0:\n",
    "                continue\n",
    "\n",
    "            batch_len = int(np.ceil(total_frames / n_actions))\n",
    "\n",
    "            for i, (_, act_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_len\n",
    "                batch_stop = min(batch_start + batch_len, stop_frame)\n",
    "\n",
    "                dummy_rows.append((\n",
    "                    video_id,\n",
    "                    act_row[\"agent\"],\n",
    "                    act_row[\"target\"],\n",
    "                    act_row[\"action\"],\n",
    "                    batch_start,\n",
    "                    batch_stop,\n",
    "                ))\n",
    "\n",
    "    if dummy_rows:\n",
    "        dummy_df = pd.DataFrame(\n",
    "            dummy_rows,\n",
    "            columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"],\n",
    "        )\n",
    "        submission = pd.concat([submission, dummy_df], ignore_index=True)\n",
    "        print(f\"Filled {len(dummy_rows)} dummy segments for empty videos\")\n",
    "\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ea78be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.001869Z",
     "iopub.status.busy": "2025-12-10T10:37:36.001602Z",
     "iopub.status.idle": "2025-12-10T10:37:36.014108Z",
     "shell.execute_reply": "2025-12-10T10:37:36.013498Z"
    },
    "papermill": {
     "duration": 0.019844,
     "end_time": "2025-12-10T10:37:36.015135",
     "exception": false,
     "start_time": "2025-12-10T10:37:35.995291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_multiclass(pred, meta, thresholds, window=5, min_len=3, merge_gap=3):\n",
    "    # 1) smoothing bằng rolling median\n",
    "    if window > 1:\n",
    "        pred_smoothed = pred.rolling(window=window, min_periods=1, center=True).mean()\n",
    "    else:\n",
    "        pred_smoothed = pred\n",
    "    \n",
    "    threshold_array = np.array([thresholds.get(col, 0.27) for col in pred.columns])\n",
    "    margins = pred_smoothed.values - threshold_array[None, :]\n",
    "\n",
    "    # 2) chọn action có margin lớn nhất\n",
    "    ama = np.argmax(margins, axis=1)              # index action tốt nhất\n",
    "    max_margin = margins[np.arange(len(ama)), ama]\n",
    "\n",
    "    # Nếu max_margin < 0 => không action nào vượt ngưỡng -> gán -1\n",
    "    ama = np.where(max_margin >= 0.0, ama, -1)\n",
    "    ama = pd.Series(ama, index=meta.video_frame)\n",
    "\n",
    "    # 3) detect change points\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    \n",
    "    mask = ama_changes.values >= 0\n",
    "    # guard: nếu không có frame nào >=0 thì trả về rỗng\n",
    "    if mask.size == 0 or mask.sum() == 0:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    mask[-1] = False  # frame cuối chỉ dùng để xác định stop cho frame trước\n",
    "    \n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id':   meta_changes['video_id'][mask].values,\n",
    "        'agent_id':   meta_changes['agent_id'][mask].values,\n",
    "        'target_id':  meta_changes['target_id'][mask].values,\n",
    "        'action':     pred.columns[ama_changes[mask].values],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame':  ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    stop_video_id  = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id  = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "    \n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "\n",
    "        if i < len(stop_video_id):\n",
    "            if (\n",
    "                stop_video_id[i] != video_id\n",
    "                or stop_agent_id[i] != agent_id\n",
    "                or stop_target_id[i] != target_id\n",
    "            ):\n",
    "                new_stop_frame = meta.query(\"video_id == @video_id\").video_frame.max() + 1\n",
    "                submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "        else:\n",
    "            new_stop_frame = meta.query(\"video_id == @video_id\").video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "\n",
    "    # 4) FILTER TRƯỚC: bỏ đoạn quá ngắn (likely noise)\n",
    "    if len(submission_part) == 0:\n",
    "        return submission_part\n",
    "\n",
    "    duration = submission_part.stop_frame - submission_part.start_frame\n",
    "    submission_part = submission_part[duration >= min_len].reset_index(drop=True)\n",
    "\n",
    "    # nếu filter xong rỗng thì trả luôn\n",
    "    if len(submission_part) == 0:\n",
    "        return submission_part\n",
    "\n",
    "    # 5) MERGE SAU: merge các đoạn gần nhau cùng (video, agent, target, action)\n",
    "    if merge_gap > 0:\n",
    "        merged = []\n",
    "        for (_, grp) in submission_part.groupby([\"video_id\", \"agent_id\", \"target_id\", \"action\"]):\n",
    "            grp = grp.sort_values(\"start_frame\").reset_index(drop=True)\n",
    "            if len(grp) == 0:\n",
    "                continue\n",
    "\n",
    "            cur_start = grp.loc[0, \"start_frame\"]\n",
    "            cur_stop  = grp.loc[0, \"stop_frame\"]\n",
    "    \n",
    "            for i in range(1, len(grp)):\n",
    "                s = grp.loc[i, \"start_frame\"]\n",
    "                e = grp.loc[i, \"stop_frame\"]\n",
    "                if s - cur_stop <= merge_gap:\n",
    "                    # merge\n",
    "                    cur_stop = max(cur_stop, e)\n",
    "                else:\n",
    "                    merged.append((grp.video_id.iloc[0],\n",
    "                                   grp.agent_id.iloc[0],\n",
    "                                   grp.target_id.iloc[0],\n",
    "                                   grp.action.iloc[0],\n",
    "                                   cur_start, cur_stop))\n",
    "                    cur_start, cur_stop = s, e\n",
    "    \n",
    "            merged.append((grp.video_id.iloc[0],\n",
    "                           grp.agent_id.iloc[0],\n",
    "                           grp.target_id.iloc[0],\n",
    "                           grp.action.iloc[0],\n",
    "                           cur_start, cur_stop))\n",
    "    \n",
    "        submission_part = pd.DataFrame(\n",
    "            merged,\n",
    "            columns=[\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "\n",
    "    return submission_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ad53be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.026918Z",
     "iopub.status.busy": "2025-12-10T10:37:36.026453Z",
     "iopub.status.idle": "2025-12-10T10:37:36.032313Z",
     "shell.execute_reply": "2025-12-10T10:37:36.031574Z"
    },
    "papermill": {
     "duration": 0.013008,
     "end_time": "2025-12-10T10:37:36.033575",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.020567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_ensemble_predictions(oof_pred_probs, y_action):\n",
    "    def objective(trial):\n",
    "        weights = [trial.suggest_float(model, -1, 1) for model in oof_pred_probs.keys()]\n",
    "        weights /= np.sum(weights)\n",
    "    \n",
    "        pred_probs = np.zeros((oof_pred_probs[list(oof_pred_probs.keys())[0]].shape[0], ))\n",
    "        for model, weight in zip(oof_pred_probs.keys(), weights):\n",
    "            pred_probs += oof_pred_probs[model] * weight\n",
    "        \n",
    "        threshold = trial.suggest_float(\"threshold\", 0, 1)\n",
    "        return f1_score(y_action, pred_probs >= threshold, zero_division=0)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1000, n_jobs=-1)\n",
    "\n",
    "    best_weights = [study.best_params[model] for model in oof_pred_probs.keys()]\n",
    "    best_weights /= np.sum(best_weights)\n",
    "    \n",
    "    return {\n",
    "        \"threshold\": study.best_params[\"threshold\"],\n",
    "        \"weight\": best_weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b05cc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.045063Z",
     "iopub.status.busy": "2025-12-10T10:37:36.044842Z",
     "iopub.status.idle": "2025-12-10T10:37:36.055205Z",
     "shell.execute_reply": "2025-12-10T10:37:36.054413Z"
    },
    "papermill": {
     "duration": 0.017588,
     "end_time": "2025-12-10T10:37:36.056403",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.038815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def cross_validate_classifier(X, label, meta, body_parts_str, section):\n",
    "    oof = pd.DataFrame(index=meta.index)\n",
    "    \n",
    "    f1_list = []\n",
    "    submission_list = []\n",
    "    thresholds = {}\n",
    "    weights = {}\n",
    "    \n",
    "    # iter by action (binary)\n",
    "    for action in label.columns:\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        X_action = X[action_mask]\n",
    "        groups_action = meta.video_id[action_mask]\n",
    "\n",
    "        # not enough video for K-fold\n",
    "        if len(np.unique(groups_action)) < CFG.n_splits:\n",
    "            print(f\"\\tSkip (not enough groups). Section: {section} Action: {action}\")\n",
    "            continue\n",
    "\n",
    "        # if all label is 0 -> not meaningful\n",
    "        if (y_action == 0).all():\n",
    "            oof_action = np.zeros(len(y_action), dtype=float)\n",
    "            print(f\"\\tF1: 0.0000 (0.00) Section: {section} Action: {action}\")\n",
    "        else:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "                    \n",
    "                    model_names = [\n",
    "                        \"xgboost3_gpu\",\n",
    "                        \"xgboost4_gpu\",\n",
    "                        \"xgboost5_gpu\"\n",
    "                    ]\n",
    "                    \n",
    "                    oof_pred_probs = {}\n",
    "                    for model_name in model_names:\n",
    "                        file_path = f\"{CFG.model_path}/{model_name}/{section}/{action}/oof_pred_probs.pkl\"\n",
    "                        if os.path.exists(file_path):\n",
    "                            oof_pred_probs[model_name] = joblib.load(file_path)\n",
    "                        else:\n",
    "                            oof_pred_probs[model_name] = np.zeros(len(y_action))\n",
    "\n",
    "                    res = optimize_ensemble_predictions(oof_pred_probs, y_action)\n",
    "                    \n",
    "                    oof_action = np.zeros((oof_pred_probs[list(oof_pred_probs.keys())[0]].shape[0], ))\n",
    "                    for model, weight in zip(oof_pred_probs.keys(), res[\"weight\"]):\n",
    "                        oof_action += oof_pred_probs[model] * weight\n",
    "\n",
    "                    threshold = res[\"threshold\"]\n",
    "                    weights[action] = res[\"weight\"]\n",
    "                    thresholds[action] = threshold\n",
    "            \n",
    "                    f1 = f1_score(y_action, (oof_action >= threshold).astype(int), zero_division=0)\n",
    "                    f1_list.append((body_parts_str, action, f1))\n",
    "\n",
    "                    print(f\"\\tF1: {f1:.4f} (thr={threshold:.2f}) Section: {section} Action: {action}\")\n",
    "\n",
    "                    del oof_pred_probs, res, threshold\n",
    "                    gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                oof_action = np.zeros(len(y_action), dtype=float)\n",
    "                print(f\"\\tERROR: {e} -> F1: 0.0000 (0.00) Section: {section} Action: {action}\")\n",
    "\n",
    "        oof_column = np.zeros(len(label), dtype=float)\n",
    "        oof_column[action_mask] = oof_action\n",
    "        oof[action] = oof_column\n",
    "\n",
    "        del oof_action, action_mask, X_action, y_action, groups_action\n",
    "        gc.collect()\n",
    "\n",
    "    submission_part = predict_multiclass(oof, meta, thresholds)\n",
    "    submission_list.append(submission_part)\n",
    "\n",
    "    return submission_list, f1_list, thresholds, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858913cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.068126Z",
     "iopub.status.busy": "2025-12-10T10:37:36.067938Z",
     "iopub.status.idle": "2025-12-10T10:37:36.075412Z",
     "shell.execute_reply": "2025-12-10T10:37:36.074667Z"
    },
    "papermill": {
     "duration": 0.0148,
     "end_time": "2025-12-10T10:37:36.076583",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.061783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(test_subset, fps_lookup, body_parts, mode, section, thresholds, weights=None):\n",
    "    sample_gen = generate_mouse_data(test_subset, mode, is_train=False)\n",
    "    submission_list = []\n",
    "\n",
    "    for sample_mode, track_df, meta_df, actions in sample_gen:\n",
    "        assert sample_mode == mode\n",
    "\n",
    "        try:\n",
    "            fps = _fps_from_meta(meta_df, fps_lookup, default_fps=30)\n",
    "            if sample_mode == \"single\":\n",
    "                X_te = transform_single(track_df, body_parts, fps)\n",
    "            else:\n",
    "                X_te = transform_pair(track_df, body_parts, fps)\n",
    "\n",
    "            del track_df\n",
    "            gc.collect()\n",
    "\n",
    "            pred = pd.DataFrame(index=meta_df.video_frame)\n",
    "\n",
    "            for action in actions:\n",
    "                model_preds = []\n",
    "                action_weights = weights.get(action)\n",
    "                if action_weights is None:\n",
    "                    continue  # skip\n",
    "\n",
    "                model_paths = {\n",
    "                    \"xgboost3_gpu\": f\"{CFG.model_path}/xgboost3_gpu/{section}/{action}\",\n",
    "                    \"xgboost4_gpu\": f\"{CFG.model_path}/xgboost4_gpu/{section}/{action}\",\n",
    "                    \"xgboost5_gpu\": f\"{CFG.model_path}/xgboost5_gpu/{section}/{action}\"\n",
    "                }\n",
    "\n",
    "                for model_name, model_path in model_paths.items():\n",
    "                    model_file = glob.glob(f\"{model_path}/*_trainer_*.pkl\")\n",
    "                    if len(model_file) == 1:\n",
    "                        trainer = joblib.load(model_file[0])\n",
    "                        model_preds.append(trainer.predict(X_te))\n",
    "                        del trainer\n",
    "                        gc.collect()\n",
    "                    else:\n",
    "                        model_preds.append(np.zeros(X_te.shape[0]))  # fallback\n",
    "\n",
    "                # weighted sum\n",
    "                pred[action] = sum(w * p for w, p in zip(action_weights, model_preds))\n",
    "\n",
    "            del X_te\n",
    "            gc.collect()\n",
    "\n",
    "            if pred.shape[1] > 0:\n",
    "                submission = predict_multiclass(pred, meta_df, thresholds)\n",
    "                submission_list.append(submission)\n",
    "\n",
    "        except KeyError:\n",
    "            del track_df\n",
    "            gc.collect()\n",
    "\n",
    "    return submission_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e56522e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.087863Z",
     "iopub.status.busy": "2025-12-10T10:37:36.087639Z",
     "iopub.status.idle": "2025-12-10T10:37:36.102719Z",
     "shell.execute_reply": "2025-12-10T10:37:36.101960Z"
    },
    "papermill": {
     "duration": 0.022167,
     "end_time": "2025-12-10T10:37:36.103957",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.081790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == \"validate\":\n",
    "    thresholds = {\n",
    "        \"single\": {},\n",
    "        \"pair\": {}\n",
    "    }\n",
    "    weights = {\n",
    "        \"single\": {},\n",
    "        \"pair\": {}\n",
    "    }\n",
    "else:\n",
    "    thresholds = joblib.load(f\"{CFG.model_path}/ensemble_345/thresholds.pkl\")\n",
    "    weights = joblib.load(f\"{CFG.model_path}/ensemble_345/weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddf648c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.115420Z",
     "iopub.status.busy": "2025-12-10T10:37:36.115236Z",
     "iopub.status.idle": "2025-12-10T10:37:36.126328Z",
     "shell.execute_reply": "2025-12-10T10:37:36.125737Z"
    },
    "papermill": {
     "duration": 0.018104,
     "end_time": "2025-12-10T10:37:36.127314",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.109210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_mode(mode, subset, body_parts, fps_lookup, section, thresholds, weights, f1_list, submission_list):\n",
    "    # validate or test\n",
    "    if CFG.mode == \"validate\":\n",
    "        data_list, label_list, meta_list = [], [], []\n",
    "    \n",
    "        for switch, data, meta, label in generate_mouse_data(subset):\n",
    "            if switch != mode:\n",
    "                continue\n",
    "            data_list.append(data)\n",
    "            meta_list.append(meta)\n",
    "            label_list.append(label)\n",
    "            del data, meta, label\n",
    "        gc.collect()\n",
    "    \n",
    "        if len(data_list) == 0:\n",
    "            return  # no sample for this mode\n",
    "    \n",
    "        # features for each sample\n",
    "        feats_parts = []\n",
    "        for data_i, meta_i in zip(data_list, meta_list):\n",
    "            fps_i = _fps_from_meta(meta_i, fps_lookup, default_fps=30.0)\n",
    "            if mode == \"single\":\n",
    "                X_i = transform_single(data_i, body_parts, fps_i)\n",
    "            else:\n",
    "                X_i = transform_pair(data_i, body_parts, fps_i)\n",
    "                \n",
    "            feats_parts.append(X_i.astype(np.float32))\n",
    "            del X_i, fps_i\n",
    "        gc.collect()\n",
    "    \n",
    "        X_tr = pd.concat(feats_parts, axis=0, ignore_index=True)\n",
    "        y_tr = pd.concat(label_list, axis=0, ignore_index=True)\n",
    "        meta_tr = pd.concat(meta_list, axis=0, ignore_index=True)\n",
    "    \n",
    "        del feats_parts, data_list, label_list, meta_list\n",
    "        gc.collect()\n",
    "    \n",
    "        temp_sub_list, temp_f1_list, temp_thr, temp_w = cross_validate_classifier(X_tr, y_tr, meta_tr, body_parts_str=str(body_parts), section=section)\n",
    "\n",
    "        # save thresholds\n",
    "        if str(section) not in thresholds[mode]:\n",
    "            thresholds[mode][str(section)] = {}\n",
    "        thresholds[mode][str(section)].update(temp_thr)\n",
    "\n",
    "        # save weights\n",
    "        if str(section) not in weights[mode]:\n",
    "            weights[mode][str(section)] = {}\n",
    "        weights[mode][str(section)].update(temp_w)\n",
    "        \n",
    "\n",
    "        f1_list.extend(temp_f1_list)\n",
    "        submission_list.extend(temp_sub_list)\n",
    "\n",
    "        del temp_sub_list, temp_f1_list, temp_thr, temp_w, X_tr\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        section_thresholds = thresholds[mode].get(str(section), {})\n",
    "        section_weights = weights[mode].get(str(section), {})\n",
    "\n",
    "        print(f\"[INFO] section={section} | thresholds len = {len(section_thresholds)} | weights len = {len(section_weights)}\")\n",
    "        \n",
    "        if not section_thresholds:\n",
    "            print(f\"[WARN] Missing thresholds for mode={mode}, section={section}. Using default=empty dict.\")\n",
    "            \n",
    "            section_thresholds = {}\n",
    "    \n",
    "        if not section_weights:\n",
    "            print(f\"[WARN] Missing weights for mode={mode}, section={section}. Using default=empty dict.\")\n",
    "            section_weights = {}\n",
    "    \n",
    "        for k, v in section_thresholds.items():\n",
    "            if v is None or (isinstance(v, float) and (v != v)):  # NaN check\n",
    "                print(f\"[WARN] Threshold '{k}' in section {section} is invalid ({v}). Setting to 0.\")\n",
    "                section_thresholds[k] = 0.0\n",
    "            else:\n",
    "                print(f\"[INFO] Threshold '{k}' in section {section} has value of ({v}).\")\n",
    "                \n",
    "    \n",
    "        for k, v in section_weights.items():\n",
    "            if v is None or (isinstance(v, float) and (v != v)):\n",
    "                print(f\"[WARN] Weight '{k}' in section {section} is invalid ({v}). Setting to 1.\")\n",
    "                section_weights[k] = 1.0\n",
    "            else:\n",
    "                print(f\"[INFO] Weight '{k}' in section {section} has value of ({v}).\")\n",
    "        \n",
    "        temp_sub_list = submit(test_subset=subset, fps_lookup=fps_lookup, body_parts=body_parts, mode=mode, section=section, thresholds=section_thresholds, weights=section_weights)\n",
    "        submission_list.extend(temp_sub_list)\n",
    "\n",
    "        del temp_sub_list\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1210b97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:37:36.138775Z",
     "iopub.status.busy": "2025-12-10T10:37:36.138539Z",
     "iopub.status.idle": "2025-12-10T10:39:33.614053Z",
     "shell.execute_reply": "2025-12-10T10:39:33.613157Z"
    },
    "papermill": {
     "duration": 117.483506,
     "end_time": "2025-12-10T10:39:33.616101",
     "exception": false,
     "start_time": "2025-12-10T10:37:36.132595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', 'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "\n",
      "[INFO] section=1 | thresholds len = 1 | weights len = 1\n",
      "[INFO] Threshold 'rear' in section 1 has value of (0.18124751788379836).\n",
      "[INFO] Weight 'rear' in section 1 has value of ([ 0.39281996  0.72423724 -0.1170572 ]).\n",
      "[INFO] section=1 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'approach' in section 1 has value of (0.15224701027015639).\n",
      "[INFO] Threshold 'attack' in section 1 has value of (0.07090925320060291).\n",
      "[INFO] Threshold 'avoid' in section 1 has value of (0.24324612257373307).\n",
      "[INFO] Threshold 'chase' in section 1 has value of (0.17321824146917764).\n",
      "[INFO] Threshold 'chaseattack' in section 1 has value of (0.193813862601314).\n",
      "[INFO] Threshold 'submit' in section 1 has value of (0.09464745949857639).\n",
      "[INFO] Weight 'approach' in section 1 has value of ([ 0.38010407  0.69378469 -0.07388876]).\n",
      "[INFO] Weight 'attack' in section 1 has value of ([ 0.68416399 -0.08006301  0.39589902]).\n",
      "[INFO] Weight 'avoid' in section 1 has value of ([0.02749572 0.90985176 0.06265252]).\n",
      "[INFO] Weight 'chase' in section 1 has value of ([-0.50227487  1.42491962  0.07735525]).\n",
      "[INFO] Weight 'chaseattack' in section 1 has value of ([ 0.61787284 -0.23445171  0.61657887]).\n",
      "[INFO] Weight 'submit' in section 1 has value of ([ 1.29751244  1.32522997 -1.62274241]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "2/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'hip_left', 'hip_right', 'lateral_left', 'lateral_right', 'nose', 'spine_1', 'spine_2', 'tail_base', 'tail_middle_1', 'tail_middle_2', 'tail_tip']\n",
      "\n",
      "[INFO] section=2 | thresholds len = 1 | weights len = 1\n",
      "[INFO] Threshold 'huddle' in section 2 has value of (0.2188099142986069).\n",
      "[INFO] Weight 'huddle' in section 2 has value of ([ 0.5086276   0.52792181 -0.03654942]).\n",
      "[INFO] section=2 | thresholds len = 2 | weights len = 2\n",
      "[INFO] Threshold 'reciprocalsniff' in section 2 has value of (0.3132526379326889).\n",
      "[INFO] Threshold 'sniffgenital' in section 2 has value of (0.2934356509690046).\n",
      "[INFO] Weight 'reciprocalsniff' in section 2 has value of ([-0.61930094  0.79884838  0.82045255]).\n",
      "[INFO] Weight 'sniffgenital' in section 2 has value of ([0.45746195 0.40121256 0.14132549]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "3/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "\n",
      "[INFO] section=3 | thresholds len = 1 | weights len = 1\n",
      "[INFO] Threshold 'rear' in section 3 has value of (0.11777772020653611).\n",
      "[INFO] Weight 'rear' in section 3 has value of ([0.70432824 0.26156781 0.03410395]).\n",
      "[INFO] section=3 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'approach' in section 3 has value of (0.035174849774712864).\n",
      "[INFO] Threshold 'attack' in section 3 has value of (0.07310213303076062).\n",
      "[INFO] Threshold 'avoid' in section 3 has value of (0.08363169236053695).\n",
      "[INFO] Threshold 'chase' in section 3 has value of (0.04315417907728479).\n",
      "[INFO] Threshold 'chaseattack' in section 3 has value of (0.07742020707960856).\n",
      "[INFO] Threshold 'submit' in section 3 has value of (0.022059891672328746).\n",
      "[INFO] Weight 'approach' in section 3 has value of ([ 0.97440486 -0.07067519  0.09627033]).\n",
      "[INFO] Weight 'attack' in section 3 has value of ([0.40645012 0.50024206 0.09330782]).\n",
      "[INFO] Weight 'avoid' in section 3 has value of ([-1.26671004  1.43934107  0.82736897]).\n",
      "[INFO] Weight 'chase' in section 3 has value of ([0.26915551 0.49366393 0.23718056]).\n",
      "[INFO] Weight 'chaseattack' in section 3 has value of ([0.26602924 0.6022999  0.13167086]).\n",
      "[INFO] Weight 'submit' in section 3 has value of ([0.94596374 0.04874592 0.00529034]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "4/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base', 'tail_tip']\n",
      "\n",
      "[INFO] section=4 | thresholds len = 0 | weights len = 0\n",
      "[WARN] Missing thresholds for mode=single, section=4. Using default=empty dict.\n",
      "[WARN] Missing weights for mode=single, section=4. Using default=empty dict.\n",
      "[INFO] section=4 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'attack' in section 4 has value of (0.1254558764118846).\n",
      "[INFO] Threshold 'dominance' in section 4 has value of (0.2506368063479895).\n",
      "[INFO] Threshold 'sniff' in section 4 has value of (0.17001847410970314).\n",
      "[INFO] Threshold 'chase' in section 4 has value of (0.06820090008414392).\n",
      "[INFO] Threshold 'escape' in section 4 has value of (0.10143565957278948).\n",
      "[INFO] Threshold 'follow' in section 4 has value of (0.3306132086779986).\n",
      "[INFO] Weight 'attack' in section 4 has value of ([0.82666622 0.10276729 0.07056649]).\n",
      "[INFO] Weight 'dominance' in section 4 has value of ([ 0.26686932  0.88709764 -0.15396696]).\n",
      "[INFO] Weight 'sniff' in section 4 has value of ([0.42504755 0.5671668  0.00778564]).\n",
      "[INFO] Weight 'chase' in section 4 has value of ([-0.13871668  0.49937815  0.63933853]).\n",
      "[INFO] Weight 'escape' in section 4 has value of ([-0.03465574  1.04097604 -0.00632029]).\n",
      "[INFO] Weight 'follow' in section 4 has value of ([0.27318668 0.52316474 0.20364858]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "5/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base']\n",
      "\n",
      "[INFO] section=5 | thresholds len = 0 | weights len = 0\n",
      "[WARN] Missing thresholds for mode=single, section=5. Using default=empty dict.\n",
      "[WARN] Missing weights for mode=single, section=5. Using default=empty dict.\n",
      "[INFO] section=5 | thresholds len = 5 | weights len = 5\n",
      "[INFO] Threshold 'attack' in section 5 has value of (0.3439687091055769).\n",
      "[INFO] Threshold 'sniff' in section 5 has value of (0.4134768064654768).\n",
      "[INFO] Threshold 'defend' in section 5 has value of (0.3220773602631692).\n",
      "[INFO] Threshold 'escape' in section 5 has value of (0.26991607295753955).\n",
      "[INFO] Threshold 'mount' in section 5 has value of (0.25393738829057483).\n",
      "[INFO] Weight 'attack' in section 5 has value of ([ 0.80525783  0.36031937 -0.1655772 ]).\n",
      "[INFO] Weight 'sniff' in section 5 has value of ([ 0.79059987  0.23581931 -0.02641919]).\n",
      "[INFO] Weight 'defend' in section 5 has value of ([0.24711083 0.50444324 0.24844593]).\n",
      "[INFO] Weight 'escape' in section 5 has value of ([0.45127634 0.29989459 0.24882906]).\n",
      "[INFO] Weight 'mount' in section 5 has value of ([ 1.176998    0.32892634 -0.50592434]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "6/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'nose', 'tail_base']\n",
      "\n",
      "[INFO] section=6 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'biteobject' in section 6 has value of (0.06617042799173203).\n",
      "[INFO] Threshold 'climb' in section 6 has value of (0.17877072957534218).\n",
      "[INFO] Threshold 'dig' in section 6 has value of (0.179387340847213).\n",
      "[INFO] Threshold 'exploreobject' in section 6 has value of (0.086326707269256).\n",
      "[INFO] Threshold 'rear' in section 6 has value of (0.11787236895385114).\n",
      "[INFO] Threshold 'selfgroom' in section 6 has value of (0.17647070964967027).\n",
      "[INFO] Weight 'biteobject' in section 6 has value of ([0.48007615 0.47717383 0.04275002]).\n",
      "[INFO] Weight 'climb' in section 6 has value of ([-0.03321574  0.21934717  0.81386857]).\n",
      "[INFO] Weight 'dig' in section 6 has value of ([0.61687497 0.29027343 0.09285161]).\n",
      "[INFO] Weight 'exploreobject' in section 6 has value of ([-1.07354933  0.87191763  1.2016317 ]).\n",
      "[INFO] Weight 'rear' in section 6 has value of ([-0.68022191  1.24567523  0.43454668]).\n",
      "[INFO] Weight 'selfgroom' in section 6 has value of ([0.0775927  0.59206918 0.33033811]).\n",
      "[INFO] section=6 | thresholds len = 12 | weights len = 12\n",
      "[INFO] Threshold 'shepherd' in section 6 has value of (0.17225009388070556).\n",
      "[INFO] Threshold 'approach' in section 6 has value of (0.20472404526734553).\n",
      "[INFO] Threshold 'attack' in section 6 has value of (0.2363460921588694).\n",
      "[INFO] Threshold 'chase' in section 6 has value of (0.1765849569033232).\n",
      "[INFO] Threshold 'defend' in section 6 has value of (0.1750087971651279).\n",
      "[INFO] Threshold 'escape' in section 6 has value of (0.3415779687714649).\n",
      "[INFO] Threshold 'flinch' in section 6 has value of (0.0348339589009341).\n",
      "[INFO] Threshold 'follow' in section 6 has value of (0.11682432529076696).\n",
      "[INFO] Threshold 'sniff' in section 6 has value of (0.2390436028794284).\n",
      "[INFO] Threshold 'sniffface' in section 6 has value of (0.30639035271273607).\n",
      "[INFO] Threshold 'sniffgenital' in section 6 has value of (0.6642589308700971).\n",
      "[INFO] Threshold 'tussle' in section 6 has value of (0.09037936240292005).\n",
      "[INFO] Weight 'shepherd' in section 6 has value of ([0.19115229 0.25046878 0.55837893]).\n",
      "[INFO] Weight 'approach' in section 6 has value of ([ 0.91520037  0.1036063  -0.01880667]).\n",
      "[INFO] Weight 'attack' in section 6 has value of ([ 0.83713603  0.23404097 -0.071177  ]).\n",
      "[INFO] Weight 'chase' in section 6 has value of ([0.41557815 0.17667255 0.40774929]).\n",
      "[INFO] Weight 'defend' in section 6 has value of ([0.30835646 0.22447364 0.4671699 ]).\n",
      "[INFO] Weight 'escape' in section 6 has value of ([0.17470765 0.60927174 0.21602061]).\n",
      "[INFO] Weight 'flinch' in section 6 has value of ([0.49461062 0.13269345 0.37269593]).\n",
      "[INFO] Weight 'follow' in section 6 has value of ([-0.58914132  0.23495775  1.35418356]).\n",
      "[INFO] Weight 'sniff' in section 6 has value of ([0.3118101  0.38830525 0.29988465]).\n",
      "[INFO] Weight 'sniffface' in section 6 has value of ([0.20797835 0.68045319 0.11156846]).\n",
      "[INFO] Weight 'sniffgenital' in section 6 has value of ([ -6.69103238  26.69984335 -19.00881096]).\n",
      "[INFO] Weight 'tussle' in section 6 has value of ([ 0.48442214  0.85168202 -0.33610416]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "7/9 Processing videos with: ['ear_left', 'ear_right', 'head', 'tail_base']\n",
      "\n",
      "[INFO] section=7 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'rear' in section 7 has value of (0.2822353822744985).\n",
      "[INFO] Threshold 'rest' in section 7 has value of (0.24202180607339308).\n",
      "[INFO] Threshold 'selfgroom' in section 7 has value of (0.16552965851986803).\n",
      "[INFO] Threshold 'climb' in section 7 has value of (0.104925648323107).\n",
      "[INFO] Threshold 'dig' in section 7 has value of (0.19056899811766567).\n",
      "[INFO] Threshold 'run' in section 7 has value of (0.10936327598710528).\n",
      "[INFO] Weight 'rear' in section 7 has value of ([0.62205431 0.14946562 0.22848007]).\n",
      "[INFO] Weight 'rest' in section 7 has value of ([ 0.55013173  0.55888209 -0.10901382]).\n",
      "[INFO] Weight 'selfgroom' in section 7 has value of ([-0.57385165  0.68191157  0.89194008]).\n",
      "[INFO] Weight 'climb' in section 7 has value of ([0.00198333 0.89103476 0.10698191]).\n",
      "[INFO] Weight 'dig' in section 7 has value of ([0.33632748 0.56701904 0.09665348]).\n",
      "[INFO] Weight 'run' in section 7 has value of ([-0.25456213  2.09679876 -0.84223663]).\n",
      "[INFO] section=7 | thresholds len = 6 | weights len = 6\n",
      "[INFO] Threshold 'sniff' in section 7 has value of (0.3504603819202004).\n",
      "[INFO] Threshold 'sniffgenital' in section 7 has value of (0.29169617610365695).\n",
      "[INFO] Threshold 'approach' in section 7 has value of (0.21958147369574044).\n",
      "[INFO] Threshold 'defend' in section 7 has value of (0.03666960075456682).\n",
      "[INFO] Threshold 'escape' in section 7 has value of (0.12698851806028738).\n",
      "[INFO] Threshold 'attemptmount' in section 7 has value of (0.0728609406408339).\n",
      "[INFO] Weight 'sniff' in section 7 has value of ([0.35131854 0.3975707  0.25111076]).\n",
      "[INFO] Weight 'sniffgenital' in section 7 has value of ([0.28709404 0.53314646 0.1797595 ]).\n",
      "[INFO] Weight 'approach' in section 7 has value of ([0.52434277 0.45661467 0.01904256]).\n",
      "[INFO] Weight 'defend' in section 7 has value of ([-0.34487859 -0.0981517   1.44303029]).\n",
      "[INFO] Weight 'escape' in section 7 has value of ([-0.05006023  0.8915277   0.15853253]).\n",
      "[INFO] Weight 'attemptmount' in section 7 has value of ([-0.76409738  0.26809814  1.49599923]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "8/9 Processing videos with: ['ear_left', 'ear_right', 'hip_left', 'hip_right', 'neck', 'nose', 'tail_base']\n",
      "\n",
      "[INFO] section=8 | thresholds len = 4 | weights len = 4\n",
      "[INFO] Threshold 'rear' in section 8 has value of (0.128761210632095).\n",
      "[INFO] Threshold 'selfgroom' in section 8 has value of (0.1150227801468841).\n",
      "[INFO] Threshold 'genitalgroom' in section 8 has value of (0.22137485729221568).\n",
      "[INFO] Threshold 'dig' in section 8 has value of (0.07854256512621612).\n",
      "[INFO] Weight 'rear' in section 8 has value of ([0.44552307 0.28363822 0.27083871]).\n",
      "[INFO] Weight 'selfgroom' in section 8 has value of ([0.37176737 0.3650173  0.26321533]).\n",
      "[INFO] Weight 'genitalgroom' in section 8 has value of ([ 0.67983148 -0.21819451  0.53836304]).\n",
      "[INFO] Weight 'dig' in section 8 has value of ([0.01571225 0.45687016 0.52741759]).\n",
      "[INFO] section=8 | thresholds len = 17 | weights len = 17\n",
      "[INFO] Threshold 'approach' in section 8 has value of (0.21527470226262005).\n",
      "[INFO] Threshold 'attack' in section 8 has value of (0.33851393526516455).\n",
      "[INFO] Threshold 'disengage' in section 8 has value of (0.2448808804059975).\n",
      "[INFO] Threshold 'mount' in section 8 has value of (0.34128336337213744).\n",
      "[INFO] Threshold 'sniff' in section 8 has value of (0.3376681014423344).\n",
      "[INFO] Threshold 'sniffgenital' in section 8 has value of (0.27825092581351096).\n",
      "[INFO] Threshold 'dominancemount' in section 8 has value of (0.22000108750842387).\n",
      "[INFO] Threshold 'sniffbody' in section 8 has value of (0.26671531559774764).\n",
      "[INFO] Threshold 'sniffface' in section 8 has value of (0.31661430965920656).\n",
      "[INFO] Threshold 'attemptmount' in section 8 has value of (0.057121900241421296).\n",
      "[INFO] Threshold 'intromit' in section 8 has value of (0.34234717113894353).\n",
      "[INFO] Threshold 'chase' in section 8 has value of (0.06657814383436707).\n",
      "[INFO] Threshold 'escape' in section 8 has value of (0.20059649163707582).\n",
      "[INFO] Threshold 'reciprocalsniff' in section 8 has value of (0.32927670455272445).\n",
      "[INFO] Threshold 'allogroom' in section 8 has value of (0.07837696792244626).\n",
      "[INFO] Threshold 'ejaculate' in section 8 has value of (0.19352052122594862).\n",
      "[INFO] Threshold 'dominancegroom' in section 8 has value of (0.10482187200903711).\n",
      "[INFO] Weight 'approach' in section 8 has value of ([0.39238071 0.31419727 0.29342202]).\n",
      "[INFO] Weight 'attack' in section 8 has value of ([ 0.66836095 -0.25554733  0.58718638]).\n",
      "[INFO] Weight 'disengage' in section 8 has value of ([0.40628462 0.42108232 0.17263307]).\n",
      "[INFO] Weight 'mount' in section 8 has value of ([0.40621778 0.03471733 0.55906489]).\n",
      "[INFO] Weight 'sniff' in section 8 has value of ([ 0.66036144 -0.26379522  0.60343378]).\n",
      "[INFO] Weight 'sniffgenital' in section 8 has value of ([0.57075589 0.28322663 0.14601748]).\n",
      "[INFO] Weight 'dominancemount' in section 8 has value of ([0.49158432 0.26216558 0.2462501 ]).\n",
      "[INFO] Weight 'sniffbody' in section 8 has value of ([ 0.6149177  -0.04130558  0.42638788]).\n",
      "[INFO] Weight 'sniffface' in section 8 has value of ([0.87849211 0.03578443 0.08572346]).\n",
      "[INFO] Weight 'attemptmount' in section 8 has value of ([ 0.82529673 -0.04214583  0.2168491 ]).\n",
      "[INFO] Weight 'intromit' in section 8 has value of ([ 1.17799565 -0.06029499 -0.11770066]).\n",
      "[INFO] Weight 'chase' in section 8 has value of ([-0.26231139  0.57255215  0.68975925]).\n",
      "[INFO] Weight 'escape' in section 8 has value of ([0.26331158 0.70901959 0.02766884]).\n",
      "[INFO] Weight 'reciprocalsniff' in section 8 has value of ([0.27566939 0.32475024 0.39958037]).\n",
      "[INFO] Weight 'allogroom' in section 8 has value of ([ 1.16086061  0.73730959 -0.8981702 ]).\n",
      "[INFO] Weight 'ejaculate' in section 8 has value of ([-0.06801825  0.97403427  0.09398397]).\n",
      "[INFO] Weight 'dominancegroom' in section 8 has value of ([ 0.85772377  1.15793053 -1.0156543 ]).\n",
      "Length of submission_list: 16\n",
      "\n",
      "9/9 Processing videos with: ['ear_left', 'ear_right', 'nose', 'tail_base', 'tail_tip']\n",
      "\n",
      "[INFO] section=9 | thresholds len = 2 | weights len = 2\n",
      "[INFO] Threshold 'freeze' in section 9 has value of (0.1944825186299171).\n",
      "[INFO] Threshold 'rear' in section 9 has value of (0.16158791265375036).\n",
      "[INFO] Weight 'freeze' in section 9 has value of ([0.95233685 0.00838999 0.03927316]).\n",
      "[INFO] Weight 'rear' in section 9 has value of ([ 0.88790043  0.39220323 -0.28010365]).\n",
      "[INFO] section=9 | thresholds len = 5 | weights len = 5\n",
      "[INFO] Threshold 'approach' in section 9 has value of (0.10193195302660865).\n",
      "[INFO] Threshold 'attack' in section 9 has value of (0.419839507367226).\n",
      "[INFO] Threshold 'defend' in section 9 has value of (0.26278532547954503).\n",
      "[INFO] Threshold 'escape' in section 9 has value of (0.4490018940555517).\n",
      "[INFO] Threshold 'sniff' in section 9 has value of (0.27553131674629633).\n",
      "[INFO] Weight 'approach' in section 9 has value of ([-1.19773803  0.55057766  1.64716037]).\n",
      "[INFO] Weight 'attack' in section 9 has value of ([0.17159895 0.56333921 0.26506184]).\n",
      "[INFO] Weight 'defend' in section 9 has value of ([0.3846757  0.44805889 0.16726541]).\n",
      "[INFO] Weight 'escape' in section 9 has value of ([ 0.70654444  0.53370121 -0.24024565]).\n",
      "[INFO] Weight 'sniff' in section 9 has value of ([0.16562447 0.5684106  0.26596492]).\n",
      "Length of submission_list: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_list = []\n",
    "submission_list = []\n",
    "import traceback\n",
    "\n",
    "for section in range(1, len(body_parts_tracked_list)):\n",
    "    body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "\n",
    "    try:\n",
    "        body_parts = json.loads(body_parts_tracked_str)\n",
    "        print(f\"{section}/{len(body_parts_tracked_list)-1} Processing videos with: {body_parts}\\n\")\n",
    "\n",
    "        if len(body_parts) > 5:\n",
    "            body_parts = [b for b in body_parts if b not in DROP_BODY_PARTS]\n",
    "\n",
    "        if CFG.mode == \"validate\":\n",
    "            subset =  train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "        else:\n",
    "            subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "\n",
    "        # lookup FPS\n",
    "        fps_lookup = (\n",
    "            subset[[\"video_id\", \"frames_per_second\"]]\n",
    "            .drop_duplicates(\"video_id\")\n",
    "            .set_index(\"video_id\")[\"frames_per_second\"]\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        # single\n",
    "        process_mode(\n",
    "            mode=\"single\",\n",
    "            subset=subset,\n",
    "            body_parts=body_parts,\n",
    "            fps_lookup=fps_lookup,\n",
    "            section=section,\n",
    "            thresholds=thresholds,\n",
    "            weights=weights,\n",
    "            f1_list=f1_list,\n",
    "            submission_list=submission_list,\n",
    "        )\n",
    "\n",
    "        # pair\n",
    "        process_mode(\n",
    "            mode=\"pair\",\n",
    "            subset=subset,\n",
    "            body_parts=body_parts,\n",
    "            fps_lookup=fps_lookup,\n",
    "            section=section,\n",
    "            thresholds=thresholds,\n",
    "            weights=weights,\n",
    "            f1_list=f1_list,\n",
    "            submission_list=submission_list,\n",
    "        )\n",
    "\n",
    "        print(f\"Length of submission_list: {len(submission_list)}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\tError: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4c53b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T10:39:33.629053Z",
     "iopub.status.busy": "2025-12-10T10:39:33.628811Z",
     "iopub.status.idle": "2025-12-10T10:39:33.671412Z",
     "shell.execute_reply": "2025-12-10T10:39:33.670826Z"
    },
    "papermill": {
     "duration": 0.050328,
     "end_time": "2025-12-10T10:39:33.672688",
     "exception": false,
     "start_time": "2025-12-10T10:39:33.622360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == 'validate':  \n",
    "    submission = pd.concat(submission_list)\n",
    "    cleaned_submission = clean_and_fill_submission(submission, train)\n",
    "    print(f\"Competition metric: {score(solution, cleaned_submission, ''):.4f}\")\n",
    "\n",
    "    f1_df = pd.DataFrame(f1_list, columns=['body_parts_tracked_str', 'action', 'binary F1 score'])\n",
    "    print(f\"Mean F1:            {f1_df['binary F1 score'].mean():.4f}\")\n",
    "  \n",
    "    os.makedirs(\"ensemble\", exist_ok=True)\n",
    "    joblib.dump(thresholds, f\"ensemble/thresholds.pkl\")\n",
    "    joblib.dump(weights, f\"ensemble/weights.pkl\")\n",
    "    joblib.dump(f1_df, f\"ensemble/scores.pkl\")\n",
    "\n",
    "elif CFG.mode == 'submit':\n",
    "    if len(submission_list) > 0:\n",
    "        submission = pd.concat(submission_list)\n",
    "    else:\n",
    "        submission = pd.DataFrame(\n",
    "            dict(\n",
    "                video_id=438887472,\n",
    "                agent_id='mouse1',\n",
    "                target_id='self',\n",
    "                action='rear',\n",
    "                start_frame=278,\n",
    "                stop_frame=500\n",
    "            ), index=[44])\n",
    "\n",
    "    cleaned_submission = clean_and_fill_submission(submission, test, is_train=False)\n",
    "    cleaned_submission.index.name = 'row_id'\n",
    "    cleaned_submission.to_csv('submission.csv')\n",
    "    submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8750188,
     "sourceId": 13751580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8738404,
     "sourceId": 14088627,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 285084080,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 132.732765,
   "end_time": "2025-12-10T10:39:34.397144",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T10:37:21.664379",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
